Treat the evaluator like a second simulation that ingests sim frames and publishes its own components. Work through `/api` (verify with `/api/status`) and begin with a clean slate by deleting any existing sim/eval systems. Upload plugins, register components, and inject systems in order; start the simulation only after those steps, and stop it when done to reset ticks and stored frames.

Visibility is asymmetric: the eval player sees full `evaluation.frame`, but the outbound eval stream filters that component. Empty entities on `/api/evaluation/stream` are expected unless your system adds something. Attach verifier outputs to a frame entity (or any entity) so they survive the filter. Include a lightweight log with counts of frames seen, entities in the last frame, and the component ids observed. If those counts stay at zero, frames aren’t reaching eval—recheck the base URL and whether the simulation is running.

Streams mirror the spec: `/api/evaluation/stream` is filtered; `/api/simulation/stream` is unfiltered. Control endpoints are POSTs for start/stop and component/system injection. A reliable pattern is: discover `/api`, clear systems, upload/register/inject, start, stream a bounded number of eval events into an artifact, then stop and inspect logs/verdicts for source component ids and nonzero counts.
